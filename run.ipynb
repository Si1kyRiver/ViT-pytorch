{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8nZ-34Ekj3h",
        "outputId": "ecdf9e86-5c0e-4100-94af-281b62be7da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ml_collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbcP885jk9Le",
        "outputId": "dddf34d1-4156-4f61-99f0-2ec65823c347"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n",
            "Building wheels for collected packages: ml_collections\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=c9f943a143d8b24337b061f5094c34a7575391acde36014079699fc44a8a8b95\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml_collections\n",
            "Installing collected packages: ml_collections\n",
            "Successfully installed ml_collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ViT-Pytorch\n",
        "!python3 train.py --name hymenoptera \\\n",
        " --dataset insects \\\n",
        " --model_type ViT-B_16 \\\n",
        " --pretrained_dir checkpoint/ViT-B_16.npz \\\n",
        " --train_batch_size 64 \\\n",
        " --num_steps 20 \\\n",
        " --eval_every 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkeaakIekqVx",
        "outputId": "e737dbb8-e4b4-4f0d-8449-7353917e18bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ViT-Pytorch\n",
            "2023-11-01 10:56:31.631840: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-01 10:56:31.631904: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-01 10:56:31.631948: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-01 10:56:31.644050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-01 10:56:33.148119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/01/2023 10:56:39 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/01/2023 10:56:58 - INFO - __main__ - classifier: token\n",
            "hidden_size: 768\n",
            "patches:\n",
            "  size: !!python/tuple\n",
            "  - 16\n",
            "  - 16\n",
            "representation_size: null\n",
            "transformer:\n",
            "  attention_dropout_rate: 0.0\n",
            "  dropout_rate: 0.1\n",
            "  mlp_dim: 3072\n",
            "  num_heads: 12\n",
            "  num_layers: 12\n",
            "\n",
            "11/01/2023 10:56:58 - INFO - __main__ - Training parameters Namespace(name='hymenoptera', dataset='insects', model_type='ViT-B_16', pretrained_dir='checkpoint/ViT-B_16.npz', output_dir='output', img_size=224, train_batch_size=64, eval_batch_size=64, eval_every=5, learning_rate=0.03, weight_decay=0, num_steps=20, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=-1, seed=42, gradient_accumulation_steps=1, fp16=False, fp16_opt_level='O2', loss_scale=0, n_gpu=1, device=device(type='cuda'))\n",
            "11/01/2023 10:56:58 - INFO - __main__ - Total Parameter: \t85.8M\n",
            "85.800194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "11/01/2023 10:57:01 - INFO - __main__ - ***** Running training *****\n",
            "11/01/2023 10:57:01 - INFO - __main__ -   Total optimization steps = 20\n",
            "11/01/2023 10:57:01 - INFO - __main__ -   Instantaneous batch size per GPU = 64\n",
            "11/01/2023 10:57:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "11/01/2023 10:57:01 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "Training (X / X Steps) (loss=X.X):   0% 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Training (1 / 20 Steps) (loss=0.69315):   0% 0/4 [00:29<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:261: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Training (4 / 20 Steps) (loss=0.69248): 100% 4/4 [00:31<00:00,  7.84s/it]\n",
            "Training (5 / 20 Steps) (loss=0.69153):   0% 0/4 [00:02<?, ?it/s]11/01/2023 10:57:34 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 10:57:34 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 10:57:34 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68984):   0% 0/3 [00:27<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68984):  33% 1/3 [00:27<00:54, 27.32s/it]\u001b[A\n",
            "Validating... (loss=0.69105):  33% 1/3 [00:27<00:54, 27.32s/it]\u001b[A\n",
            "Validating... (loss=0.69105):  67% 2/3 [00:27<00:11, 11.37s/it]\u001b[A\n",
            "Validating... (loss=0.69128): 100% 3/3 [00:27<00:00,  9.28s/it]\n",
            "11/01/2023 10:58:02 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 10:58:02 - INFO - __main__ - Validation Results\n",
            "11/01/2023 10:58:02 - INFO - __main__ - Global Steps: 5\n",
            "11/01/2023 10:58:02 - INFO - __main__ - Valid Loss: 0.69072\n",
            "11/01/2023 10:58:02 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "11/01/2023 10:58:10 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (8 / 20 Steps) (loss=0.68791): 100% 4/4 [00:39<00:00,  9.88s/it]\n",
            "Training (10 / 20 Steps) (loss=0.67985):  25% 1/4 [00:02<00:05,  1.95s/it]11/01/2023 10:58:14 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 10:58:14 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 10:58:14 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67431):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67431):  33% 1/3 [00:01<00:03,  1.85s/it]\u001b[A\n",
            "Validating... (loss=0.68085):  33% 1/3 [00:02<00:03,  1.85s/it]\u001b[A\n",
            "Validating... (loss=0.68085):  67% 2/3 [00:02<00:00,  1.12it/s]\u001b[A\n",
            "Validating... (loss=0.68214): 100% 3/3 [00:02<00:00,  1.25it/s]\n",
            "11/01/2023 10:58:17 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 10:58:17 - INFO - __main__ - Validation Results\n",
            "11/01/2023 10:58:17 - INFO - __main__ - Global Steps: 10\n",
            "11/01/2023 10:58:17 - INFO - __main__ - Valid Loss: 0.67910\n",
            "11/01/2023 10:58:17 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "11/01/2023 10:58:18 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (12 / 20 Steps) (loss=0.67159): 100% 4/4 [00:07<00:00,  1.92s/it]\n",
            "Training (15 / 20 Steps) (loss=0.65852):  50% 2/4 [00:03<00:02,  1.19s/it]11/01/2023 10:58:23 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 10:58:23 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 10:58:23 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.64429):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.64429):  33% 1/3 [00:01<00:02,  1.28s/it]\u001b[A\n",
            "Validating... (loss=0.65816):  33% 1/3 [00:01<00:02,  1.28s/it]\u001b[A\n",
            "Validating... (loss=0.65816):  67% 2/3 [00:01<00:00,  1.56it/s]\u001b[A\n",
            "Validating... (loss=0.66118): 100% 3/3 [00:01<00:00,  1.71it/s]\n",
            "11/01/2023 10:58:24 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 10:58:24 - INFO - __main__ - Validation Results\n",
            "11/01/2023 10:58:24 - INFO - __main__ - Global Steps: 15\n",
            "11/01/2023 10:58:24 - INFO - __main__ - Valid Loss: 0.65454\n",
            "11/01/2023 10:58:24 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (16 / 20 Steps) (loss=0.64826): 100% 4/4 [00:05<00:00,  1.41s/it]\n",
            "Training (20 / 20 Steps) (loss=0.61359):  75% 3/4 [00:03<00:00,  1.02it/s]11/01/2023 10:58:29 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 10:58:29 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 10:58:29 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.59879):   0% 0/3 [00:02<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.59879):  33% 1/3 [00:02<00:04,  2.13s/it]\u001b[A\n",
            "Validating... (loss=0.62473):  33% 1/3 [00:02<00:04,  2.13s/it]\u001b[A\n",
            "Validating... (loss=0.62473):  67% 2/3 [00:02<00:00,  1.01it/s]\u001b[A\n",
            "Validating... (loss=0.63039): 100% 3/3 [00:02<00:00,  1.13it/s]\n",
            "11/01/2023 10:58:32 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 10:58:32 - INFO - __main__ - Validation Results\n",
            "11/01/2023 10:58:32 - INFO - __main__ - Global Steps: 20\n",
            "11/01/2023 10:58:32 - INFO - __main__ - Valid Loss: 0.61797\n",
            "11/01/2023 10:58:32 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (20 / 20 Steps) (loss=0.61359):  75% 3/4 [00:06<00:02,  2.29s/it]\n",
            "11/01/2023 10:58:32 - INFO - __main__ - Best Accuracy: \t0.973856\n",
            "11/01/2023 10:58:32 - INFO - __main__ - End Training!\n"
          ]
        }
      ]
    }
  ]
}