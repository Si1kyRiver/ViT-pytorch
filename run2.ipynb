{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8nZ-34Ekj3h",
        "outputId": "4dcd0f55-59e5-4d06-fd96-7331b967e3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ml_collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbcP885jk9Le",
        "outputId": "581c97f5-d50d-42dc-9a20-b446b187a719"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n",
            "Building wheels for collected packages: ml_collections\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=770e8b3de5be4a6c232091d837f52d4d124deca3f667b5ea81e42ec0b6e4821f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml_collections\n",
            "Installing collected packages: ml_collections\n",
            "Successfully installed ml_collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zero_head = True\n",
        "# adjust order\n",
        "%cd /content/drive/MyDrive/ViT-Pytorch\n",
        "!python3 train.py --name hymenoptera \\\n",
        " --dataset insects \\\n",
        " --model_type ViT-B_16 \\\n",
        " --pretrained_dir checkpoint/ViT-B_16.npz \\\n",
        " --train_batch_size 32 \\\n",
        " --num_steps 20 \\\n",
        " --eval_every 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkeaakIekqVx",
        "outputId": "7ccc44f1-9ba7-432d-ce10-df2d0c2c2e18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ViT-Pytorch\n",
            "2023-11-02 00:02:25.773958: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-02 00:02:25.774029: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-02 00:02:25.774083: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-02 00:02:25.786988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-02 00:02:27.334247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/02/2023 00:02:29 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/02/2023 00:02:34 - INFO - __main__ - classifier: token\n",
            "hidden_size: 768\n",
            "patches:\n",
            "  size: !!python/tuple\n",
            "  - 16\n",
            "  - 16\n",
            "representation_size: null\n",
            "transformer:\n",
            "  attention_dropout_rate: 0.0\n",
            "  dropout_rate: 0.1\n",
            "  mlp_dim: 3072\n",
            "  num_heads: 12\n",
            "  num_layers: 12\n",
            "\n",
            "11/02/2023 00:02:34 - INFO - __main__ - Training parameters Namespace(name='hymenoptera', dataset='insects', model_type='ViT-B_16', pretrained_dir='checkpoint/ViT-B_16.npz', output_dir='output', img_size=224, train_batch_size=32, eval_batch_size=64, eval_every=5, learning_rate=0.03, weight_decay=0, num_steps=20, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=-1, seed=42, gradient_accumulation_steps=1, fp16=False, fp16_opt_level='O2', loss_scale=0, n_gpu=1, device=device(type='cuda'))\n",
            "11/02/2023 00:02:34 - INFO - __main__ - Total Parameter: \t85.8M\n",
            "85.800194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "11/02/2023 00:02:34 - INFO - __main__ - ***** Running training *****\n",
            "11/02/2023 00:02:34 - INFO - __main__ -   Total optimization steps = 20\n",
            "11/02/2023 00:02:34 - INFO - __main__ -   Instantaneous batch size per GPU = 32\n",
            "11/02/2023 00:02:34 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "11/02/2023 00:02:34 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "Training (1 / 20 Steps) (loss=0.69315):   0% 0/8 [00:03<?, ?it/s]11/02/2023 00:02:37 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:37 - INFO - __main__ - Train Acc: 0.5312\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:261: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Training (2 / 20 Steps) (loss=0.69315):  12% 1/8 [00:03<00:22,  3.27s/it]11/02/2023 00:02:38 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:38 - INFO - __main__ - Train Acc: 0.7500\n",
            "Training (3 / 20 Steps) (loss=0.69304):  25% 2/8 [00:03<00:09,  1.58s/it]11/02/2023 00:02:38 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:38 - INFO - __main__ - Train Acc: 0.8229\n",
            "Training (4 / 20 Steps) (loss=0.69276):  38% 3/8 [00:04<00:05,  1.05s/it]11/02/2023 00:02:39 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:39 - INFO - __main__ - Train Acc: 0.8672\n",
            "Training (5 / 20 Steps) (loss=0.69227):  50% 4/8 [00:04<00:03,  1.26it/s]11/02/2023 00:02:39 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:39 - INFO - __main__ - Train Acc: 0.8812\n",
            "11/02/2023 00:02:39 - INFO - __main__ - ***** Running Validation *****\n",
            "11/02/2023 00:02:39 - INFO - __main__ -   Num steps = 3\n",
            "11/02/2023 00:02:39 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69113):   0% 0/3 [00:02<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69113):  33% 1/3 [00:02<00:04,  2.00s/it]\u001b[A\n",
            "Validating... (loss=0.69148):  33% 1/3 [00:02<00:04,  2.00s/it]\u001b[A\n",
            "Validating... (loss=0.69148):  67% 2/3 [00:02<00:00,  1.06it/s]\u001b[A\n",
            "Validating... (loss=0.69159): 100% 3/3 [00:02<00:00,  1.18it/s]\n",
            "11/02/2023 00:02:42 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:42 - INFO - __main__ - Validation Results\n",
            "11/02/2023 00:02:42 - INFO - __main__ - Global Steps: 5\n",
            "11/02/2023 00:02:42 - INFO - __main__ - Valid Loss: 0.69140\n",
            "11/02/2023 00:02:42 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "11/02/2023 00:02:44 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (6 / 20 Steps) (loss=0.69125):  62% 5/8 [00:09<00:07,  2.38s/it]11/02/2023 00:02:44 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:44 - INFO - __main__ - Train Acc: 0.8854\n",
            "Training (7 / 20 Steps) (loss=0.69015):  75% 6/8 [00:10<00:03,  1.71s/it]11/02/2023 00:02:45 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:45 - INFO - __main__ - Train Acc: 0.8973\n",
            "Training (8 / 20 Steps) (loss=0.68854):  88% 7/8 [00:10<00:01,  1.29s/it]11/02/2023 00:02:45 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:45 - INFO - __main__ - Train Acc: 0.9016\n",
            "Training (8 / 20 Steps) (loss=0.68854): 100% 8/8 [00:10<00:00,  1.36s/it]\n",
            "Training (9 / 20 Steps) (loss=0.68620):   0% 0/8 [00:01<?, ?it/s]11/02/2023 00:02:46 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:46 - INFO - __main__ - Train Acc: 0.9094\n",
            "Training (10 / 20 Steps) (loss=0.68389):  12% 1/8 [00:01<00:09,  1.29s/it]11/02/2023 00:02:47 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:47 - INFO - __main__ - Train Acc: 0.9188\n",
            "11/02/2023 00:02:47 - INFO - __main__ - ***** Running Validation *****\n",
            "11/02/2023 00:02:47 - INFO - __main__ -   Num steps = 3\n",
            "11/02/2023 00:02:47 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67882):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67882):  33% 1/3 [00:01<00:02,  1.25s/it]\u001b[A\n",
            "Validating... (loss=0.68235):  33% 1/3 [00:01<00:02,  1.25s/it]\u001b[A\n",
            "Validating... (loss=0.68235):  67% 2/3 [00:01<00:00,  1.58it/s]\u001b[A\n",
            "Validating... (loss=0.68325): 100% 3/3 [00:01<00:00,  1.79it/s]\n",
            "11/02/2023 00:02:48 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:48 - INFO - __main__ - Validation Results\n",
            "11/02/2023 00:02:48 - INFO - __main__ - Global Steps: 10\n",
            "11/02/2023 00:02:48 - INFO - __main__ - Valid Loss: 0.68148\n",
            "11/02/2023 00:02:48 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (11 / 20 Steps) (loss=0.68133):  25% 2/8 [00:03<00:10,  1.77s/it]11/02/2023 00:02:49 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:49 - INFO - __main__ - Train Acc: 0.9206\n",
            "Training (12 / 20 Steps) (loss=0.67841):  38% 3/8 [00:04<00:05,  1.15s/it]11/02/2023 00:02:49 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:49 - INFO - __main__ - Train Acc: 0.9247\n",
            "Training (13 / 20 Steps) (loss=0.66844):  50% 4/8 [00:04<00:03,  1.17it/s]11/02/2023 00:02:50 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:50 - INFO - __main__ - Train Acc: 0.9307\n",
            "Training (14 / 20 Steps) (loss=0.66792):  62% 5/8 [00:04<00:02,  1.44it/s]11/02/2023 00:02:50 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:50 - INFO - __main__ - Train Acc: 0.9358\n",
            "Training (15 / 20 Steps) (loss=0.66709):  75% 6/8 [00:05<00:01,  1.67it/s]11/02/2023 00:02:50 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:50 - INFO - __main__ - Train Acc: 0.9338\n",
            "11/02/2023 00:02:50 - INFO - __main__ - ***** Running Validation *****\n",
            "11/02/2023 00:02:50 - INFO - __main__ -   Num steps = 3\n",
            "11/02/2023 00:02:50 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.65159):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.65159):  33% 1/3 [00:01<00:02,  1.44s/it]\u001b[A\n",
            "Validating... (loss=0.66262):  33% 1/3 [00:01<00:02,  1.44s/it]\u001b[A\n",
            "Validating... (loss=0.66262):  67% 2/3 [00:01<00:00,  1.41it/s]\u001b[A\n",
            "Validating... (loss=0.66528): 100% 3/3 [00:01<00:00,  1.57it/s]\n",
            "11/02/2023 00:02:52 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:52 - INFO - __main__ - Validation Results\n",
            "11/02/2023 00:02:52 - INFO - __main__ - Global Steps: 15\n",
            "11/02/2023 00:02:52 - INFO - __main__ - Valid Loss: 0.65983\n",
            "11/02/2023 00:02:52 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (16 / 20 Steps) (loss=0.65454):  88% 7/8 [00:07<00:01,  1.16s/it]11/02/2023 00:02:53 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:53 - INFO - __main__ - Train Acc: 0.9344\n",
            "Training (16 / 20 Steps) (loss=0.65454): 100% 8/8 [00:07<00:00,  1.02it/s]\n",
            "Training (17 / 20 Steps) (loss=0.64824):   0% 0/8 [00:02<?, ?it/s]11/02/2023 00:02:55 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:55 - INFO - __main__ - Train Acc: 0.9385\n",
            "Training (18 / 20 Steps) (loss=0.64348):  12% 1/8 [00:02<00:17,  2.51s/it]11/02/2023 00:02:56 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:56 - INFO - __main__ - Train Acc: 0.9420\n",
            "Training (19 / 20 Steps) (loss=0.63763):  25% 2/8 [00:03<00:07,  1.31s/it]11/02/2023 00:02:56 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:56 - INFO - __main__ - Train Acc: 0.9401\n",
            "Training (20 / 20 Steps) (loss=0.62779):  38% 3/8 [00:03<00:04,  1.10it/s]11/02/2023 00:02:57 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:57 - INFO - __main__ - Train Acc: 0.9416\n",
            "11/02/2023 00:02:57 - INFO - __main__ - ***** Running Validation *****\n",
            "11/02/2023 00:02:57 - INFO - __main__ -   Num steps = 3\n",
            "11/02/2023 00:02:57 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.60903):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.60903):  33% 1/3 [00:01<00:02,  1.22s/it]\u001b[A\n",
            "Validating... (loss=0.63280):  33% 1/3 [00:01<00:02,  1.22s/it]\u001b[A\n",
            "Validating... (loss=0.63280):  67% 2/3 [00:01<00:00,  1.62it/s]\u001b[A\n",
            "Validating... (loss=0.63816): 100% 3/3 [00:01<00:00,  1.84it/s]\n",
            "11/02/2023 00:02:58 - INFO - __main__ - \n",
            "\n",
            "11/02/2023 00:02:58 - INFO - __main__ - Validation Results\n",
            "11/02/2023 00:02:58 - INFO - __main__ - Global Steps: 20\n",
            "11/02/2023 00:02:58 - INFO - __main__ - Valid Loss: 0.62667\n",
            "11/02/2023 00:02:58 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "Training (20 / 20 Steps) (loss=0.62779):  38% 3/8 [00:05<00:09,  1.87s/it]\n",
            "11/02/2023 00:02:59 - INFO - __main__ - Best Accuracy: \t0.973856\n",
            "11/02/2023 00:02:59 - INFO - __main__ - End Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not adjust order:\n",
        "%cd /content/drive/MyDrive/ViT-Pytorch\n",
        "!python3 train.py --name hymenoptera \\\n",
        " --dataset insects \\\n",
        " --model_type ViT-B_16 \\\n",
        " --pretrained_dir checkpoint/ViT-B_16.npz \\\n",
        " --train_batch_size 32 \\\n",
        " --num_steps 10 \\\n",
        " --eval_every 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZgPQpNPM7P8",
        "outputId": "48904407-23be-4cc6-ca1a-94a1c3fa2a68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ViT-Pytorch\n",
            "2023-11-01 23:41:27.209804: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-01 23:41:27.209863: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-01 23:41:27.209907: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-01 23:41:27.217692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-01 23:41:28.242777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/01/2023 23:41:30 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/01/2023 23:41:36 - INFO - __main__ - classifier: token\n",
            "hidden_size: 768\n",
            "patches:\n",
            "  size: !!python/tuple\n",
            "  - 16\n",
            "  - 16\n",
            "representation_size: null\n",
            "transformer:\n",
            "  attention_dropout_rate: 0.0\n",
            "  dropout_rate: 0.1\n",
            "  mlp_dim: 3072\n",
            "  num_heads: 12\n",
            "  num_layers: 12\n",
            "\n",
            "11/01/2023 23:41:36 - INFO - __main__ - Training parameters Namespace(name='hymenoptera', dataset='insects', model_type='ViT-B_16', pretrained_dir='checkpoint/ViT-B_16.npz', output_dir='output', img_size=224, train_batch_size=32, eval_batch_size=64, eval_every=2, learning_rate=0.03, weight_decay=0, num_steps=10, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=-1, seed=42, gradient_accumulation_steps=1, fp16=False, fp16_opt_level='O2', loss_scale=0, n_gpu=1, device=device(type='cuda'))\n",
            "11/01/2023 23:41:36 - INFO - __main__ - Total Parameter: \t85.8M\n",
            "85.800194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "11/01/2023 23:41:36 - INFO - __main__ - ***** Running training *****\n",
            "11/01/2023 23:41:36 - INFO - __main__ -   Total optimization steps = 10\n",
            "11/01/2023 23:41:36 - INFO - __main__ -   Instantaneous batch size per GPU = 32\n",
            "11/01/2023 23:41:36 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "11/01/2023 23:41:36 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "Training (X / X Steps) (loss=X.X):   0% 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Training (1 / 10 Steps) (loss=0.69315):   0% 0/8 [00:03<?, ?it/s]11/01/2023 23:41:39 - INFO - __main__ - Train Acc: 1.0000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:261: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Training (2 / 10 Steps) (loss=0.69309):  12% 1/8 [00:03<00:23,  3.40s/it]11/01/2023 23:41:40 - INFO - __main__ - Train Acc: 0.9844\n",
            "11/01/2023 23:41:40 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 23:41:40 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 23:41:40 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69281):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69281):  33% 1/3 [00:01<00:02,  1.23s/it]\u001b[A\n",
            "Validating... (loss=0.69295):  33% 1/3 [00:01<00:02,  1.23s/it]\u001b[A\n",
            "Validating... (loss=0.69295):  67% 2/3 [00:01<00:00,  1.59it/s]\u001b[A\n",
            "Validating... (loss=0.69298): 100% 3/3 [00:01<00:00,  1.81it/s]\n",
            "11/01/2023 23:41:42 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 23:41:42 - INFO - __main__ - Validation Results\n",
            "11/01/2023 23:41:42 - INFO - __main__ - Global Steps: 2\n",
            "11/01/2023 23:41:42 - INFO - __main__ - Valid Loss: 0.69291\n",
            "11/01/2023 23:41:42 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "11/01/2023 23:41:43 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (3 / 10 Steps) (loss=0.69286):  25% 2/8 [00:06<00:19,  3.23s/it]11/01/2023 23:41:43 - INFO - __main__ - Train Acc: 0.9792\n",
            "Training (4 / 10 Steps) (loss=0.69247):  38% 3/8 [00:07<00:09,  1.94s/it]11/01/2023 23:41:43 - INFO - __main__ - Train Acc: 0.9844\n",
            "11/01/2023 23:41:43 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 23:41:43 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 23:41:43 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69139):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69139):  33% 1/3 [00:01<00:02,  1.22s/it]\u001b[A\n",
            "Validating... (loss=0.69194):  33% 1/3 [00:01<00:02,  1.22s/it]\u001b[A\n",
            "Validating... (loss=0.69194):  67% 2/3 [00:01<00:00,  1.62it/s]\u001b[A\n",
            "Validating... (loss=0.69206): 100% 3/3 [00:01<00:00,  1.75it/s]\n",
            "11/01/2023 23:41:45 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 23:41:45 - INFO - __main__ - Validation Results\n",
            "11/01/2023 23:41:45 - INFO - __main__ - Global Steps: 4\n",
            "11/01/2023 23:41:45 - INFO - __main__ - Valid Loss: 0.69180\n",
            "11/01/2023 23:41:45 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "11/01/2023 23:41:46 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (5 / 10 Steps) (loss=0.69183):  50% 4/8 [00:10<00:10,  2.56s/it]11/01/2023 23:41:47 - INFO - __main__ - Train Acc: 0.9750\n",
            "Training (6 / 10 Steps) (loss=0.69050):  62% 5/8 [00:11<00:05,  1.78s/it]11/01/2023 23:41:47 - INFO - __main__ - Train Acc: 0.9635\n",
            "11/01/2023 23:41:47 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 23:41:47 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 23:41:47 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68860):   0% 0/3 [00:02<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68860):  33% 1/3 [00:02<00:04,  2.05s/it]\u001b[A\n",
            "Validating... (loss=0.68960):  33% 1/3 [00:02<00:04,  2.05s/it]\u001b[A\n",
            "Validating... (loss=0.68960):  67% 2/3 [00:02<00:00,  1.05it/s]\u001b[A\n",
            "Validating... (loss=0.68985): 100% 3/3 [00:02<00:00,  1.22it/s]\n",
            "11/01/2023 23:41:50 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 23:41:50 - INFO - __main__ - Validation Results\n",
            "11/01/2023 23:41:50 - INFO - __main__ - Global Steps: 6\n",
            "11/01/2023 23:41:50 - INFO - __main__ - Valid Loss: 0.68935\n",
            "11/01/2023 23:41:50 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (7 / 10 Steps) (loss=0.68921):  75% 6/8 [00:14<00:04,  2.15s/it]11/01/2023 23:41:50 - INFO - __main__ - Train Acc: 0.9643\n",
            "Training (8 / 10 Steps) (loss=0.68733):  88% 7/8 [00:14<00:01,  1.58s/it]11/01/2023 23:41:50 - INFO - __main__ - Train Acc: 0.9631\n",
            "11/01/2023 23:41:50 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 23:41:50 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 23:41:50 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68367):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68367):  33% 1/3 [00:01<00:02,  1.14s/it]\u001b[A\n",
            "Validating... (loss=0.68586):  33% 1/3 [00:01<00:02,  1.14s/it]\u001b[A\n",
            "Validating... (loss=0.68586):  67% 2/3 [00:01<00:00,  1.61it/s]\u001b[A\n",
            "Validating... (loss=0.68641): 100% 3/3 [00:01<00:00,  1.85it/s]\n",
            "11/01/2023 23:41:52 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 23:41:52 - INFO - __main__ - Validation Results\n",
            "11/01/2023 23:41:52 - INFO - __main__ - Global Steps: 8\n",
            "11/01/2023 23:41:52 - INFO - __main__ - Valid Loss: 0.68532\n",
            "11/01/2023 23:41:52 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (8 / 10 Steps) (loss=0.68733): 100% 8/8 [00:16<00:00,  2.02s/it]\n",
            "Training (9 / 10 Steps) (loss=0.68423):   0% 0/8 [00:01<?, ?it/s]11/01/2023 23:41:54 - INFO - __main__ - Train Acc: 0.9674\n",
            "Training (10 / 10 Steps) (loss=0.68258):  12% 1/8 [00:01<00:09,  1.42s/it]11/01/2023 23:41:54 - INFO - __main__ - Train Acc: 0.9708\n",
            "11/01/2023 23:41:54 - INFO - __main__ - ***** Running Validation *****\n",
            "11/01/2023 23:41:54 - INFO - __main__ -   Num steps = 3\n",
            "11/01/2023 23:41:54 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67678):   0% 0/3 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67678):  33% 1/3 [00:01<00:02,  1.42s/it]\u001b[A\n",
            "Validating... (loss=0.68020):  33% 1/3 [00:01<00:02,  1.42s/it]\u001b[A\n",
            "Validating... (loss=0.68020):  67% 2/3 [00:01<00:00,  1.43it/s]\u001b[A\n",
            "Validating... (loss=0.68115): 100% 3/3 [00:01<00:00,  1.61it/s]\n",
            "11/01/2023 23:41:56 - INFO - __main__ - \n",
            "\n",
            "11/01/2023 23:41:56 - INFO - __main__ - Validation Results\n",
            "11/01/2023 23:41:56 - INFO - __main__ - Global Steps: 10\n",
            "11/01/2023 23:41:56 - INFO - __main__ - Valid Loss: 0.67938\n",
            "11/01/2023 23:41:56 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (10 / 10 Steps) (loss=0.68258):  12% 1/8 [00:03<00:27,  3.90s/it]\n",
            "11/01/2023 23:41:56 - INFO - __main__ - Best Accuracy: \t0.973856\n",
            "11/01/2023 23:41:56 - INFO - __main__ - End Training!\n"
          ]
        }
      ]
    }
  ]
}